{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Databricks\n",
    "\n",
    "CREATE TABLE cases\n",
    "USING csv\n",
    "OPTIONS (path \"/databricks-datasets/COVID/coronavirusdataset/Case.csv\", header \"true\") \n",
    "\n",
    "CREATE TABLE PatientInfo\n",
    "USING csv\n",
    "OPTIONS (path \"/databricks-datasets/COVID/coronavirusdataset/PatientInfo.csv\", header \"true\") \n",
    "\n",
    "CREATE TABLE PatientRoute\n",
    "USING csv\n",
    "OPTIONS (path \"/databricks-datasets/COVID/coronavirusdataset/PatientRoute.csv\", header \"true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bceba7f6-2f9f-434a-afa7-c97f578c83fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questão 1: 174\n",
      "Questão 2: group possui apenas 2 valores únicos\n",
      "Questão 3: 65.48850574712644\n",
      "Questão 4: 4511\n",
      "Questão 5: 33.68421052631579\n",
      "Questão 6: 5165\n",
      "Questão 7: state possui apenas 3 valores únicos\n",
      "Questão 8: 2\n",
      "Questão 9: 10410\n",
      "Questão 10: 38.19317\n",
      "Questão 11: 126.301\n",
      "Questão 12: 20\n",
      "Questão 13: O Spark compartilha dados entre clusters através do Spark Dataframe e por isso, esse é o tipo recomendado a se trabalhar.\n",
      "Questão 14: JavaScript\n",
      "Questão 15: O Spark oferece grande vantagem, processando em memória RAM e ainda podendo ser incorporado a notebooks \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Criar uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Desafio_Modulo_4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler o arquivo CSV e criar um DataFrame\n",
    "caminho_arquivo = '/databricks-datasets/COVID/coronavirusdataset/Case.csv'\n",
    "df = spark.read.csv(caminho_arquivo, header=True, inferSchema=True)\n",
    "\n",
    "caminho_arquivo = '/databricks-datasets/COVID/coronavirusdataset/PatientInfo.csv'\n",
    "df_1 = spark.read.csv(caminho_arquivo, header=True, inferSchema=True)\n",
    "\n",
    "caminho_arquivo = '/databricks-datasets/COVID/coronavirusdataset/PatientRoute.csv'\n",
    "df_2 = spark.read.csv(caminho_arquivo, header=True, inferSchema=True)\n",
    "\n",
    "# Contar o número de linhas no DataFrame\n",
    "numero_de_linhas = df.count()\n",
    "\n",
    "# Exibir o número de linhas\n",
    "print(\"Questão 1:\", numero_de_linhas)\n",
    "\n",
    "# Questão 2\n",
    "contagem_distinta_q2 = df.select('group').distinct().count()\n",
    "print(\"Questão 2:\", \"group possui apenas\", contagem_distinta_q2,\"valores únicos\")\n",
    "\n",
    "# Questão 3 \n",
    "questao_3 = (\n",
    "df\n",
    " .select(['confirmed'])\n",
    " .describe()\n",
    " .select('confirmed')\n",
    " .where(\"summary = 'mean'\")\n",
    " .collect()[0]['confirmed'])\n",
    "print(\"Questão 3:\", questao_3)\n",
    "\n",
    "# Questão 4\n",
    "df_q4 = df.orderBy(F.col('confirmed').desc()).limit(1)\n",
    "df_q4_2 = df_q4.select('confirmed').collect()[0]['confirmed']\n",
    "print(\"Questão 4:\", df_q4_2)\n",
    "\n",
    "#Questão 5\n",
    "df_seoul = df.filter(F.col('province') == 'Seoul')\n",
    "media_confirmed_seoul = df_seoul.agg(F.avg('confirmed').alias('media_confirmed_seoul')).collect()[0]['media_confirmed_seoul']\n",
    "print(\"Questão 5:\", media_confirmed_seoul)\n",
    "\n",
    "\n",
    "# Questão 6 \n",
    "numero_de_linhas = df_1.count()\n",
    "print(\"Questão 6:\", numero_de_linhas)\n",
    "\n",
    "contagem_distinta_q7 = df_1.select('state').distinct().count()\n",
    "\n",
    "# Questão 7\n",
    "print(\"Questão 7:\", \"state possui apenas\", contagem_distinta_q7,\"valores únicos\")\n",
    "\n",
    "# Questão 8 \n",
    "Questao_8 = (\n",
    "    df_1\n",
    "    .filter(\n",
    "        (F.col(\"sex\") == \"female\") &\n",
    "        (F.col(\"city\") == \"Jongno-gu\") &\n",
    "        (F.col(\"age\") == \"10s\")\n",
    "    ).select(F.count('*').alias('count')).collect()[0]['count'])\n",
    "\n",
    "print(\"Questão 8:\", Questao_8)\n",
    "\n",
    "# Questão 9\n",
    "numero_de_linhas = df_2.count()\n",
    "print(\"Questão 9:\", numero_de_linhas)\n",
    "\n",
    "# Questão 10\n",
    "df_q10 = df_2.orderBy(F.col('latitude').desc()).limit(1)\n",
    "df_q10_2 = df_q10.select('latitude').collect()[0]['latitude']\n",
    "print(\"Questão 10:\", df_q10_2)\n",
    "\n",
    "# Questão 11\n",
    "df_q11 = df_2.orderBy(F.col('longitude').asc()).limit(1)\n",
    "df_q11_2 = df_q11.select('longitude').collect()[0]['longitude']\n",
    "print(\"Questão 11:\", df_q11_2)\n",
    "\n",
    "# Questão 12\n",
    "# Filtrar o DataFrame para incluir apenas as linhas onde 'province' é igual a 'Seoul'\n",
    "df_type_airport = df_2.filter(F.col('type') == 'airport')\n",
    "df_w_pat = df_1.filter(F.col('infection_case') == 'contact with patient')\n",
    "\n",
    "df_joined = df_type_airport.join(df_w_pat, df_type_airport.patient_id == df_w_pat.patient_id, 'inner')\n",
    "\n",
    "print(\"Questão 12:\", df_joined.count())\n",
    "print(\"Questão 13:\",\"O Spark compartilha dados entre clusters através do Spark Dataframe e por isso, esse é o tipo recomendado a se trabalhar.\")\n",
    "print(\"Questão 14:\",\"JavaScript\")\n",
    "print(\"Questão 15:\",\"O Spark oferece grande vantagem, processando em memória RAM e ainda podendo ser incorporado a notebooks \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d98765-ffac-4253-a9a2-6ff8d562fe7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 216000449180131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Teste",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
